{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block 2.1\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Getting Structured Data from the Internet:</h1>\n",
    "\n",
    "<h2>Running Web Crawlers/Scrapers on a Big Data Production Scale</h2>\n",
    "\n",
    "\n",
    "<p id = \"first\">\n",
    "Jay M. Patel\n",
    "</p>\n",
    "\n",
    "</body>\n",
    "\n",
    "</html> \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block after inserting styling (complete code not shown in book)\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\" style=\"color:green;\">Getting Structured Data from the Internet:</h1>\n",
    "\n",
    "<h2>Running Web Crawlers/Scrapers on a Big Data Production Scale</h2>\n",
    "\n",
    "\n",
    "<p id = \"first\">\n",
    "Jay M. Patel\n",
    "</p>\n",
    "\n",
    "</body>\n",
    "\n",
    "</html> \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "********************\n",
      "['ASCII_SPACES', 'DEFAULT_BUILDER_FEATURES', 'HTML_FORMATTERS', 'NO_PARSER_SPECIFIED_WARNING', 'ROOT_TAG_NAME', 'XML_FORMATTERS', '__bool__', '__call__', '__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_strings', '_attr_value_as_string', '_attribute_checker', '_check_markup_is_url', '_feed', '_find_all', '_find_one', '_formatter_for_name', '_is_xml', '_lastRecursiveChild', '_last_descendant', '_most_recent_element', '_popToTag', '_select_debug', '_selector_combinators', '_should_pretty_print', '_tag_name_matches_and', 'append', 'attribselect_re', 'attrs', 'builder', 'can_be_empty_element', 'childGenerator', 'children', 'clear', 'contains_replacement_characters', 'contents', 'currentTag', 'current_data', 'declared_html_encoding', 'decode', 'decode_contents', 'decompose', 'descendants', 'encode', 'encode_contents', 'endData', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'get', 'getText', 'get_attribute_list', 'get_text', 'handle_data', 'handle_endtag', 'handle_starttag', 'has_attr', 'has_key', 'hidden', 'index', 'insert', 'insert_after', 'insert_before', 'isSelfClosing', 'is_empty_element', 'is_xml', 'known_xml', 'markup', 'name', 'namespace', 'new_string', 'new_tag', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_element', 'next_elements', 'next_sibling', 'next_siblings', 'object_was_parsed', 'original_encoding', 'parent', 'parentGenerator', 'parents', 'parse_only', 'parserClass', 'parser_class', 'popTag', 'prefix', 'preserve_whitespace_tag_stack', 'preserve_whitespace_tags', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_element', 'previous_elements', 'previous_sibling', 'previous_siblings', 'pushTag', 'quoted_colon', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'reset', 'select', 'select_one', 'setup', 'string', 'strings', 'stripped_strings', 'tagStack', 'tag_name_re', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "# Code block 2.2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "test_url = 'https://web.archive.org/web/20200331040501/https://en.wikipedia.org/wiki/Web_scraping'\n",
    "\n",
    "r = requests.get(test_url)\n",
    "html_response = r.text\n",
    "# creating a beautifulsoup object\n",
    "soup = BeautifulSoup(html_response,'html.parser')\n",
    "print(type(soup))\n",
    "print(\"*\"*20)\n",
    "print(dir(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "********************\n",
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Web scraping</h1>\n",
      "********************\n",
      "firstHeading\n",
      "********************\n",
      "{'id': 'firstHeading', 'class': ['firstHeading'], 'lang': 'en'}\n"
     ]
    }
   ],
   "source": [
    "# Code block 2.3\n",
    "\n",
    "first_tag = (soup.h1)\n",
    "print(type(first_tag))\n",
    "print(\"*\"*20)\n",
    "print(first_tag)\n",
    "print(\"*\"*20)\n",
    "print(first_tag[\"id\"])\n",
    "print(\"*\"*20)\n",
    "print(first_tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.NavigableString'>\n",
      "********************\n",
      "<class 'str'> Web scraping\n",
      "********************\n",
      "<class 'str'> Web scraping\n"
     ]
    }
   ],
   "source": [
    "# Code block 2.4\n",
    "\n",
    "first_string = first_tag.string\n",
    "print(type(first_string))\n",
    "print(\"*\"*20)\n",
    "python_string = str(first_string)\n",
    "print(type(python_string), python_string)\n",
    "print(\"*\"*20)\n",
    "print(type(first_tag.get_text()), first_tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping\n",
      "Contents\n",
      "History[edit]\n",
      "Techniques[edit]\n",
      "Software[edit]\n",
      "Legal issues[edit]\n",
      "Methods to prevent web scraping[edit]\n",
      "See also[edit]\n",
      "References[edit]\n",
      "Navigation menu\n",
      "********************\n",
      "Web scraping\n"
     ]
    }
   ],
   "source": [
    "# code block 2.5\n",
    "# Passing a list to find_all method\n",
    "for object in soup.find_all(['h1', 'h2']):\n",
    "    print(object.get_text())\n",
    "# doing the same to find()\n",
    "print(\"*\"*20)\n",
    "print(soup.find(['h1','h2']).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Archive.is': '/web/20200331040501/https://en.wikipedia.org/wiki/Archive.is', 'Comparison of feed aggregators': '/web/20200331040501/https://en.wikipedia.org/wiki/Comparison_of_feed_aggregators', 'Data scraping': '/web/20200331040501/https://en.wikipedia.org/wiki/Data_scraping', 'Data wrangling': '/web/20200331040501/https://en.wikipedia.org/wiki/Data_wrangling', 'Importer': '/web/20200331040501/https://en.wikipedia.org/wiki/Importer_(computing)', 'Job wrapping': '/web/20200331040501/https://en.wikipedia.org/wiki/Job_wrapping', 'Knowledge extraction': '/web/20200331040501/https://en.wikipedia.org/wiki/Knowledge_extraction', 'OpenSocial': '/web/20200331040501/https://en.wikipedia.org/wiki/OpenSocial', 'Scraper site': '/web/20200331040501/https://en.wikipedia.org/wiki/Scraper_site', 'Fake news website': '/web/20200331040501/https://en.wikipedia.org/wiki/Fake_news_website', 'Blog scraping': '/web/20200331040501/https://en.wikipedia.org/wiki/Blog_scraping', 'Spamdexing': '/web/20200331040501/https://en.wikipedia.org/wiki/Spamdexing', 'Domain name drop list': '/web/20200331040501/https://en.wikipedia.org/wiki/Domain_name_drop_list', 'Text corpus': '/web/20200331040501/https://en.wikipedia.org/wiki/Text_corpus', 'Web archiving': '/web/20200331040501/https://en.wikipedia.org/wiki/Web_archiving', 'Blog network': '/web/20200331040501/https://en.wikipedia.org/wiki/Blog_network', 'Search Engine Scraping': '/web/20200331040501/https://en.wikipedia.org/wiki/Search_Engine_Scraping', 'Web crawlers': '/web/20200331040501/https://en.wikipedia.org/wiki/Category:Web_crawlers'}\n"
     ]
    }
   ],
   "source": [
    "# code block 2.6\n",
    "\n",
    "link_div = soup.find('div', {'class':'div-col columns column-width'})\n",
    "link_dict = {}\n",
    "links = link_div.find_all('a')\n",
    "\n",
    "for link in links:\n",
    "    anchor_text = link.get_text()\n",
    "    link_dict[anchor_text] = link['href']\n",
    "print(link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\n\\n\\nRobot Check\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnter the characters you see below\\nSorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nType the characters you see in this image:\\n\\n\\n\\n\\n\\n\\n\\n\\nTry different image\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContinue shopping\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConditions of Use\\n\\n\\n\\n\\nPrivacy Policy\\n\\n\\n          Â© 1996-2014, Amazon.com, Inc. or its affiliates\\n          \\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.7\n",
    "\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "}\n",
    "\n",
    "url = 'https://www.amazon.com'\n",
    "rr = requests.get(url, headers = my_headers)\n",
    "ht_response = rr.text\n",
    "soup = BeautifulSoup(ht_response,'html.parser')\n",
    "for script in soup([\"script\"]): \n",
    "        script.extract()\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Robot Check\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnter the characters you see below\\nSorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nType the characters you see in this image:\\n\\n\\n\\n\\n\\n\\n\\n\\nTry different image\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContinue shopping\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConditions of Use\\n\\n\\n\\n\\nPrivacy Policy\\n\\n\\n          Â© 1996-2014, Amazon.com, Inc. or its affiliates\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for script in soup([\"script\"]): \n",
    "        script.extract()\n",
    "soup.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484240267', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484238288', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484243657', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484249406', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484236482', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484232873', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484244791', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484236758', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484242452', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484240144', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484236390', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484244500', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484235188', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484239124', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484242230', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484251249#otherversion=9781484251249', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484256503#otherversion=9781484256503', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484255315#otherversion=9781484255315', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484255063#otherversion=9781484255063', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484255698#otherversion=9781484255698', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484255810#otherversion=9781484255810', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484255582#otherversion=9781484255582', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484253700#otherversion=9781484253700', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484253281#otherversion=9781484253281', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484254615#otherversion=9781484254615', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484251621#otherversion=9781484251621', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484255773#otherversion=9781484255773', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484253496#otherversion=9781484253496', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484254820#otherversion=9781484254820', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484251942#otherversion=9781484251942', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484226001', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484233658', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484242339', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484249710', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484241271', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484243428', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484237717', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484242933', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484236697', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484252833', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484254189', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484234341', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484235515', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484239872', 'https://web.archive.org/web/20200219120507/web/20200219120507/https://www.apress.com/us/book/9781484249789']\n"
     ]
    }
   ],
   "source": [
    "# code block 2.8\n",
    "\n",
    "url = 'https://web.archive.org/web/20200219120507/https://www.apress.com/us/shop'\n",
    "base_url = 'https://web.archive.org/web/20200219120507'\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "}\n",
    "r = requests.get(url, headers = my_headers)\n",
    "ht_response = r.text\n",
    "\n",
    "soup = BeautifulSoup(ht_response,'html.parser')\n",
    "\n",
    "product_info = soup.find_all(\"div\", {\"class\":\"product-information\"})\n",
    "url_list =[]\n",
    "\n",
    "for product in product_info:\n",
    "    \n",
    "    temp_url = base_url + str(product.parent.find('a')[\"href\"])\n",
    "    url_list.append(temp_url)\n",
    "print(url_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extracted_products': [{'book_type': 'eBook', 'book_price': '$39.99', 'book_name': 'Pro .NET Benchmarking', 'url': 'https://web.archive.org/web/20191018112156/https://www.apress.com/us/book/9781484249406'}]}\n"
     ]
    }
   ],
   "source": [
    "# code block 2.9\n",
    "url = 'https://web.archive.org/web/20191018112156/https://www.apress.com/us/book/9781484249406'\n",
    "\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "}\n",
    "\n",
    "rr = requests.get(url, headers = my_headers)\n",
    "ht_response = rr.text\n",
    "\n",
    "temp_dict = {}\n",
    "results_list = []\n",
    "main_dict = {}\n",
    "\n",
    "soup = BeautifulSoup(ht_response,'html.parser')\n",
    "primary_buy = soup.find(\"span\", {\"class\":\"cover-type\"})\n",
    "temp_dict[\"book_type\"] = primary_buy.get_text()\n",
    "temp_dict[\"book_price\"] = primary_buy.parent.find(\"span\", {\"class\": \"price\"}).get_text().strip()\n",
    "temp_dict[\"book_name\"] = soup.find('h1').get_text()\n",
    "temp_dict[\"url\"] = url\n",
    "\n",
    "results_list.append(temp_dict)\n",
    "main_dict[\"extracted_products\"] = results_list\n",
    "\n",
    "print(main_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 19 09:39:00 2020    restats\n",
      "\n",
      "         79174 function calls (79158 primitive calls) in 0.086 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 102 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.086    0.086 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.085    0.085 <string>:2(<module>)\n",
      "        1    0.000    0.000    0.085    0.085 <string>:5(main)\n",
      "        1    0.000    0.000    0.078    0.078 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:87(__init__)\n",
      "        1    0.000    0.000    0.078    0.078 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:285(_feed)\n",
      "        1    0.000    0.000    0.078    0.078 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:210(feed)\n",
      "        1    0.000    0.000    0.078    0.078 C:\\ProgramData\\Anaconda3\\lib\\html\\parser.py:104(feed)\n",
      "        1    0.007    0.007    0.078    0.078 C:\\ProgramData\\Anaconda3\\lib\\html\\parser.py:134(goahead)\n",
      "      715    0.008    0.000    0.045    0.000 C:\\ProgramData\\Anaconda3\\lib\\html\\parser.py:301(parse_starttag)\n",
      "      715    0.003    0.000    0.027    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:79(handle_starttag)\n",
      "      715    0.002    0.000    0.023    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:447(handle_starttag)\n",
      "      619    0.003    0.000    0.015    0.000 C:\\ProgramData\\Anaconda3\\lib\\html\\parser.py:386(parse_endtag)\n",
      "     1464    0.005    0.000    0.014    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:337(endData)\n",
      "      714    0.001    0.000    0.012    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:107(handle_endtag)\n",
      "      716    0.003    0.000    0.011    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\element.py:813(__init__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x2a8520dd8d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.10\n",
    "\n",
    "import cProfile\n",
    "\n",
    "cProfile.run('''\n",
    "temp_dict = {}\n",
    "results_list = []\n",
    "main_dict = {}\n",
    "def main():\n",
    "    \n",
    "        \n",
    "        soup = BeautifulSoup(ht_response,'html.parser')\n",
    "        primary_buy = soup.find(\"span\", {\"class\":\"cover-type\"})\n",
    "        temp_dict[\"book_type\"] = primary_buy.get_text()\n",
    "        temp_dict[\"book_price\"] = primary_buy.parent.find(\"span\", {\"class\": \"price\"}).get_text().strip()\n",
    "        temp_dict[\"book_name\"] = soup.find('h1').get_text()\n",
    "        temp_dict[\"url\"] = url\n",
    "        results_list.append(temp_dict)\n",
    "\n",
    "        main_dict[\"extracted_products\"] = results_list\n",
    "        return(results_list)\n",
    "main()''', 'restats')\n",
    "\n",
    "#https://docs.python.org/3.6/library/profile.html\n",
    "import pstats\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats('cumtime').print_stats(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 19 09:39:57 2020    restats\n",
      "\n",
      "         63900 function calls (63880 primitive calls) in 0.064 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 168 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.064    0.064 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.063    0.063 <string>:2(<module>)\n",
      "        1    0.000    0.000    0.063    0.063 <string>:5(main)\n",
      "        1    0.000    0.000    0.058    0.058 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:87(__init__)\n",
      "        1    0.000    0.000    0.058    0.058 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:285(_feed)\n",
      "        1    0.000    0.000    0.058    0.058 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py:246(feed)\n",
      "      2/1    0.006    0.003    0.047    0.047 src/lxml/parser.pxi:1242(feed)\n",
      "      715    0.001    0.000    0.026    0.000 src/lxml/saxparser.pxi:374(_handleSaxTargetStartNoNs)\n",
      "      715    0.000    0.000    0.024    0.000 src/lxml/saxparser.pxi:401(_callTargetSaxStart)\n",
      "      715    0.000    0.000    0.024    0.000 src/lxml/parsertarget.pxi:78(_handleSaxStart)\n",
      "      715    0.004    0.000    0.023    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py:145(start)\n",
      "      715    0.002    0.000    0.017    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:447(handle_starttag)\n",
      "      715    0.001    0.000    0.011    0.000 src/lxml/saxparser.pxi:452(_handleSaxEndNoNs)\n",
      "     2181    0.004    0.000    0.011    0.000 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:337(endData)\n",
      "      715    0.000    0.000    0.010    0.000 src/lxml/parsertarget.pxi:84(_handleSaxEnd)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x2a852202780>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.11\n",
    "\n",
    "import cProfile\n",
    "\n",
    "cProfile.run('''\n",
    "temp_dict = {}\n",
    "results_list = []\n",
    "main_dict = {}\n",
    "def main():\n",
    "    \n",
    "        \n",
    "        soup = BeautifulSoup(ht_response,'lxml')\n",
    "        primary_buy = soup.find(\"span\", {\"class\":\"cover-type\"})\n",
    "        temp_dict[\"book_type\"] = primary_buy.get_text()\n",
    "        temp_dict[\"book_price\"] = primary_buy.parent.find(\"span\", {\"class\": \"price\"}).get_text().strip()\n",
    "        temp_dict[\"book_name\"] = soup.find('h1').get_text()\n",
    "        temp_dict[\"url\"] = url\n",
    "        results_list.append(temp_dict)\n",
    "\n",
    "        main_dict[\"extracted_products\"] = results_list\n",
    "        return(results_list)\n",
    "main()''', 'restats')\n",
    "\n",
    "#https://docs.python.org/3.6/library/profile.html\n",
    "import pstats\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats('cumtime').print_stats(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extracted_products': [{'book_name': 'Pro .NET Benchmarking',\n",
       "   'book_price': '$39.99',\n",
       "   'book_type': 'eBook',\n",
       "   'url': 'https://web.archive.org/web/20191018112156/https://www.apress.com/us/book/9781484249406'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.12\n",
    "from lxml.html import fromstring, tostring\n",
    "\n",
    "temp_dict = {}\n",
    "results_list = []\n",
    "main_dict = {}\n",
    "def main():\n",
    "    \n",
    "        \n",
    "        tree = fromstring(ht_response)\n",
    "        \n",
    "        temp_dict[\"book_type\"] = tree.xpath('//*[@id=\"content\"]/div[2]/div[2]/div[1]/div/dl/dt[1]/span[1]/text()')[0]\n",
    "        temp_dict[\"book_price\"] = tree.xpath('//*[@id=\"content\"]/div[2]/div[2]/div[1]/div/dl/dt[1]/span[2]/span/text()')[0].strip()\n",
    "        temp_dict[\"book_name\"] = tree.xpath('//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[1]/div[2]/h1/text()')[0]\n",
    "        temp_dict[\"url\"] = url\n",
    "        results_list.append(temp_dict)\n",
    "\n",
    "        main_dict[\"extracted_products\"] = results_list\n",
    "        return(main_dict)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 19 10:08:05 2020    restats\n",
      "\n",
      "         436 function calls in 0.016 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 103 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.016    0.016 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.015    0.015 <string>:2(<module>)\n",
      "        1    0.000    0.000    0.015    0.015 <string>:5(main)\n",
      "        1    0.000    0.000    0.012    0.012 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py:861(fromstring)\n",
      "        1    0.000    0.000    0.012    0.012 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py:759(document_fromstring)\n",
      "        1    0.000    0.000    0.012    0.012 src/lxml/etree.pyx:3198(fromstring)\n",
      "        1    0.007    0.007    0.007    0.007 src/lxml/etree.pyx:354(getroot)\n",
      "        1    0.000    0.000    0.005    0.005 src/lxml/parser.pxi:1869(_parseMemoryDocument)\n",
      "        1    0.000    0.000    0.005    0.005 src/lxml/parser.pxi:1731(_parseDoc)\n",
      "        1    0.005    0.005    0.005    0.005 src/lxml/parser.pxi:1009(_parseUnicodeDoc)\n",
      "        3    0.000    0.000    0.003    0.001 src/lxml/etree.pyx:1568(xpath)\n",
      "        3    0.003    0.001    0.003    0.001 src/lxml/xpath.pxi:281(__call__)\n",
      "        3    0.000    0.000    0.000    0.000 src/lxml/xpath.pxi:252(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 src/lxml/xpath.pxi:131(__init__)\n",
      "       30    0.000    0.000    0.000    0.000 src/lxml/parser.pxi:612(_forwardParserError)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x2a851ffabe0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.13\n",
    "\n",
    "from lxml.html import fromstring, tostring\n",
    "\n",
    "import cProfile\n",
    "\n",
    "cProfile.run('''\n",
    "temp_dict = {}\n",
    "results_list = []\n",
    "main_dict = {}\n",
    "def main():\n",
    "    \n",
    "        \n",
    "        tree = fromstring(ht_response)\n",
    "        \n",
    "        temp_dict[\"book_type\"] = tree.xpath('//*[@id=\"content\"]/div[2]/div[2]/div[1]/div/dl/dt[1]/span[1]/text()')[0]\n",
    "        temp_dict[\"book_price\"] = tree.xpath('//*[@id=\"content\"]/div[2]/div[2]/div[1]/div/dl/dt[1]/span[2]/span/text()')[0].strip()\n",
    "        temp_dict[\"book_name\"] = tree.xpath('//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[1]/div[2]/h1/text()')[0]\n",
    "        temp_dict[\"url\"] = url\n",
    "        results_list.append(temp_dict)\n",
    "\n",
    "        main_dict[\"extracted_products\"] = results_list\n",
    "        return(main_dict)\n",
    "main()''', 'restats')\n",
    "\n",
    "#https://docs.python.org/3.6/library/profile.html\n",
    "import pstats\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats('cumtime').print_stats(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " {'http://jaympatel.com/',\n",
       "  'http://jaympatel.com/2018/11/get-started-with-git-and-github-in-under-10-minutes/',\n",
       "  'http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification/',\n",
       "  'http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup/',\n",
       "  'http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python/',\n",
       "  'http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/',\n",
       "  'http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python/',\n",
       "  'http://jaympatel.com/2019/02/top-data-science-interview-questions-and-answers/',\n",
       "  'http://jaympatel.com/2019/02/using-twitter-rest-apis-in-python-to-search-and-download-tweets-in-bulk/',\n",
       "  'http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping/',\n",
       "  'http://jaympatel.com/2020/01/introduction-to-machine-learning-metrics/',\n",
       "  'http://jaympatel.com/about/',\n",
       "  'http://jaympatel.com/books',\n",
       "  'http://jaympatel.com/books/',\n",
       "  'http://jaympatel.com/categories/',\n",
       "  'http://jaympatel.com/categories/#data-mining',\n",
       "  'http://jaympatel.com/categories/#data-science',\n",
       "  'http://jaympatel.com/categories/#interviews',\n",
       "  'http://jaympatel.com/categories/#machine-learning',\n",
       "  'http://jaympatel.com/categories/#natural-language-processing',\n",
       "  'http://jaympatel.com/categories/#requests',\n",
       "  'http://jaympatel.com/categories/#sentiments',\n",
       "  'http://jaympatel.com/categories/#software-development',\n",
       "  'http://jaympatel.com/categories/#text-vectorization',\n",
       "  'http://jaympatel.com/categories/#twitter',\n",
       "  'http://jaympatel.com/categories/#web-scraping',\n",
       "  'http://jaympatel.com/consulting-services',\n",
       "  'http://jaympatel.com/consulting-services/',\n",
       "  'http://jaympatel.com/cv',\n",
       "  'http://jaympatel.com/cv/',\n",
       "  'http://jaympatel.com/pages/CV.pdf',\n",
       "  'http://jaympatel.com/tags/',\n",
       "  'http://jaympatel.com/tags/#coefficient-of-determination-r2',\n",
       "  'http://jaympatel.com/tags/#git',\n",
       "  'http://jaympatel.com/tags/#glove',\n",
       "  'http://jaympatel.com/tags/#information-criterion',\n",
       "  'http://jaympatel.com/tags/#language-detection',\n",
       "  'http://jaympatel.com/tags/#machine-learning',\n",
       "  'http://jaympatel.com/tags/#name-entity-recognition',\n",
       "  'http://jaympatel.com/tags/#p-value',\n",
       "  'http://jaympatel.com/tags/#regex',\n",
       "  'http://jaympatel.com/tags/#regression',\n",
       "  'http://jaympatel.com/tags/#t-test',\n",
       "  'http://jaympatel.com/tags/#term-frequency-inverse-document-frequency-tf-idf',\n",
       "  'http://jaympatel.com/tags/#text-mining',\n",
       "  'http://jaympatel.com/tags/#tweepy',\n",
       "  'http://jaympatel.com/tags/#version-control',\n",
       "  'http://jaympatel.com/tags/#web-scraping',\n",
       "  'http://jaympatel.com/tags/#word-embeddings',\n",
       "  'http://jaympatel.com/tags/#words2vec',\n",
       "  'http://www.jaympatel.com',\n",
       "  'http://www.jaympatel.com/assets/DoD_SERDP_case_study.pdf'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.14\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def link_crawler(seed_url, max_n = 5000):\n",
    "    my_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "    }\n",
    "    initial_url_set = set()\n",
    "    initial_url_set.add(seed_url)\n",
    "    seen_url_set = set()\n",
    "    \n",
    "    while len(initial_url_set)!=0 and len(seen_url_set) < max_n:\n",
    "        temp_url = initial_url_set.pop()\n",
    "        if temp_url in seen_url_set:\n",
    "            continue\n",
    "        else:\n",
    "            seen_url_set.add(temp_url)\n",
    "            r = requests.get(url = temp_url, headers = my_headers)\n",
    "            st_code = r.status_code\n",
    "            #print(st_code)\n",
    "            html_response = r.text\n",
    "            soup = BeautifulSoup(html_response,'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                #print(link['href'])\n",
    "                if ('http' in link['href']):\n",
    "                    if seed_url.split(\".\")[1] in link['href']:\n",
    "                        initial_url_set.add(link['href'])\n",
    "                        #print(link['href'])\n",
    "                elif [char for char in link['href']][0] == '/':\n",
    "                    final_url = seed_url+link['href']\n",
    "                    initial_url_set.add(final_url)\n",
    "               \n",
    "    return(initial_url_set, seen_url_set)\n",
    "seed_url = 'http://www.jaympatel.com'\n",
    "link_crawler(seed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.specrom.com'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.15\n",
    "\n",
    "from tld import get_tld\n",
    "sample_url = 'http://www.specrom.com/?utm_source=google&utm_medium=banner&utm_campaign=fall_sale&utm_term=web%20scraping%20crawling'\n",
    "\n",
    "def get_normalized_url(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    path_list = [char for char in res.parsed_url.path]\n",
    "    if len(path_list) == 0:\n",
    "        final_url = res.parsed_url.scheme+'://'+res.parsed_url.netloc\n",
    "    elif path_list[-1] == '/':\n",
    "        final_string = ''.join(path_list[:-1])\n",
    "        final_url = res.parsed_url.scheme+'://'+res.parsed_url.netloc+final_string\n",
    "    else:\n",
    "        \n",
    "        final_url = url\n",
    "    return final_url\n",
    "\n",
    "get_normalized_url(sample_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block 2.16\n",
    "\n",
    "# final robot parser code\n",
    "from urllib import robotparser\n",
    "from tld import get_tld\n",
    "\n",
    "def get_rb_object(url):\n",
    "    robot_url = get_robot_url(url)\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(robot_url)\n",
    "    rp.read()\n",
    "    return(rp)\n",
    "\n",
    "def parse_robot(url,rb_object):\n",
    "    \n",
    "    flag = rb_object.can_fetch(\"*\", url)\n",
    "    try:\n",
    "        crawl_d = rb_object.crawl_delay(\"*\")\n",
    "    except Exception as E:\n",
    "        crawl_d = None\n",
    "    return flag, crawl_d\n",
    "\n",
    "def get_robot_url(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    final_url = res.parsed_url.scheme+'://'+res.parsed_url.netloc+'/robots.txt'\n",
    "    return(final_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " Counter({'http://jaympatel.com': 18,\n",
       "          'http://jaympatel.com/2018/11/get-started-with-git-and-github-in-under-10-minutes': 55,\n",
       "          'http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification': 30,\n",
       "          'http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup': 29,\n",
       "          'http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python': 29,\n",
       "          'http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach': 35,\n",
       "          'http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python': 32,\n",
       "          'http://jaympatel.com/2019/02/top-data-science-interview-questions-and-answers': 48,\n",
       "          'http://jaympatel.com/2019/02/using-twitter-rest-apis-in-python-to-search-and-download-tweets-in-bulk': 30,\n",
       "          'http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping': 28,\n",
       "          'http://jaympatel.com/2020/01/introduction-to-machine-learning-metrics': 29,\n",
       "          'http://jaympatel.com/about': 19,\n",
       "          'http://jaympatel.com/books': 39,\n",
       "          'http://jaympatel.com/categories': 93,\n",
       "          'http://jaympatel.com/consulting-services': 21,\n",
       "          'http://jaympatel.com/cv': 2,\n",
       "          'http://jaympatel.com/pages/CV.pdf': 1,\n",
       "          'http://jaympatel.com/tags': 114,\n",
       "          'http://www.jaympatel.com': 2,\n",
       "          'http://www.jaympatel.com/assets/DoD_SERDP_case_study.pdf': 1}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.17\n",
    "# final code for advanced crawler\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tld import get_fld\n",
    "from tld import get_tld\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def advanced_link_crawler(seed_url, max_n = 5000):\n",
    "    my_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "    }\n",
    "    initial_url_set = set()\n",
    "    initial_url_list = []\n",
    "    seen_url_set = set()\n",
    "    base_url = 'http://www.'+ get_fld(seed_url)\n",
    "    \n",
    "    res = get_tld(seed_url, as_object=True)\n",
    "    domain_name = res.fld\n",
    "    \n",
    "    initial_url_set.add(seed_url)\n",
    "    initial_url_list.append(seed_url)\n",
    "    \n",
    "    robot_object = get_rb_object(seed_url)\n",
    "    flag, delay_time = parse_robot(seed_url,robot_object)\n",
    "    \n",
    "    if delay_time is None:\n",
    "        delay_time = 0.1\n",
    "    \n",
    "    if flag is False:\n",
    "        print('crawling not permitted')\n",
    "        return(initial_url_set, seen_url_set)\n",
    "    \n",
    "    while len(initial_url_set)!=0 and len(seen_url_set) < max_n:\n",
    "        temp_url = initial_url_set.pop()\n",
    "        \n",
    "        if temp_url in seen_url_set:\n",
    "            continue\n",
    "        else:\n",
    "            seen_url_set.add(temp_url)\n",
    "            \n",
    "            time.sleep(delay_time)\n",
    "            \n",
    "            r = requests.get(url = temp_url, headers = my_headers)\n",
    "            st_code = r.status_code\n",
    "            if st_code != 200:\n",
    "                time.sleep(delay_time)\n",
    "                r = requests.get(url = temp_url, headers = my_headers)\n",
    "                if r.status_code != 200:\n",
    "                    continue\n",
    "            #print(st_code)\n",
    "            html_response = r.text\n",
    "            soup = BeautifulSoup(html_response,'lxml')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                #print(link['href'])\n",
    "                if ('http' in link['href']):\n",
    "                    if domain_name in link['href']:\n",
    "                        final_url = link['href']\n",
    "                    else:\n",
    "                        continue\n",
    "                       \n",
    "                elif [char for char in link['href']][0] == '/':\n",
    "                    final_url = base_url+link['href']\n",
    "                \n",
    "                # insert url normalization\n",
    "                #print(final_url)\n",
    "                final_url = get_normalized_url(final_url)\n",
    "                flag, delay = parse_robot(seed_url,robot_object)\n",
    "                # insert robot file checking\n",
    "                if flag is True:\n",
    "                        \n",
    "                    initial_url_set.add(final_url.strip())\n",
    "                    initial_url_list.append(final_url.strip())\n",
    "    counted_dict = Counter(initial_url_list)           \n",
    "    return(initial_url_set, counted_dict)\n",
    "seed_url = 'http://www.jaympatel.com'\n",
    "advanced_link_crawler(seed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request code:  200\n",
      "<tr>\n",
      "<th>path hidden</th> <th>Posted Date</th> <th>Letter Issue Date</th> <th>Company Name</th> <th>Issuing Office hidden</th> <th>Issuing Office hidden</th> <th>Recipient Office Office hidden</th> <th>Issuing Office old hidden</th> <th>Letter Type condition hidden</th> <th>Issuing Office</th> <th>State hidden</th> <th>State hidden</th> <th>Regulated Product hidden</th> <th>Subject</th> <th>Topics hidden</th> <th>Topics and regulated Product combined hidden</th> <th>Year hidden</th> <th>Letter Type</th> <th>Response Letter</th> <th>Closeout Letter</th> <th>checkresponse hidden</th> </tr>\n"
     ]
    }
   ],
   "source": [
    "# code block 2.18\n",
    "\n",
    "# scraping US FDA without executing javascript\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "}\n",
    "\n",
    "test_url = 'https://web.archive.org/web/20200406193325/https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/compliance-actions-and-activities/warning-letters'\n",
    "\n",
    "r = requests.get(url = test_url, headers = my_headers)\n",
    "print(\"request code: \", r.status_code)\n",
    "html_response = r.text\n",
    "time.sleep(30)\n",
    "# creating a beautifulsoup object\n",
    "soup = BeautifulSoup(html_response,'lxml')\n",
    "\n",
    "for tr in soup.find_all('tr'):\n",
    "    print(tr)\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code block 2.19\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<script>\n",
    "    function changeColor() {\n",
    "    var msg;\n",
    "    msg = document.getElementById(\"firstHeading\");\n",
    "    msg.style.color = \"green\";\n",
    "    }\n",
    "</script>\n",
    "<body>\n",
    "\n",
    "<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Getting Structured Data from the Internet:</h1>\n",
    "\n",
    "<h2>Running Web Crawlers/Scrapers on a Big Data Production Scale</h2>\n",
    "\n",
    "\n",
    "<p id = \"first\">\n",
    "Jay M. Patel\n",
    "</p>\n",
    "\n",
    "<input type=\"button\" value=\"Change color!\" onclick=\"changeColor();\">\n",
    "\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. Food and Drug Administration\n",
      "U.S. Food and Drug Administration\n",
      "Warning Letters\n"
     ]
    }
   ],
   "source": [
    "# code block 2.20\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "test_url = 'https://web.archive.org/web/20200406193325/https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/compliance-actions-and-activities/warning-letters'\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\"--incognito\")\n",
    "chromedriver = r'C:\\Users\\Jay M. Patel\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(chromedriver, options=option)\n",
    "browser.get(test_url)\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "print(browser.find_element_by_tag_name('h1').text)\n",
    "element_list = browser.find_elements_by_tag_name('h1')\n",
    "for element in element_list:\n",
    "    print(element.text)\n",
    "\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>issuing ofice</th>\n",
       "      <th>letter_issue</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>warning_letter_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health Solutions Pharmacy Center Inc dba Profe...</td>\n",
       "      <td>Division of Pharmaceutical Quality Operations IV</td>\n",
       "      <td>03/31/2020</td>\n",
       "      <td>04/28/2020</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promex Distribution LLC</td>\n",
       "      <td>Division of Southeast Imports</td>\n",
       "      <td>04/13/2020</td>\n",
       "      <td>04/28/2020</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JusByJulie.Com LLC</td>\n",
       "      <td>Office of Human and Animal Food Operations Eas...</td>\n",
       "      <td>04/13/2020</td>\n",
       "      <td>04/28/2020</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T &amp; S Dairy</td>\n",
       "      <td>Division of Human and Animal Food Operations W...</td>\n",
       "      <td>04/14/2020</td>\n",
       "      <td>04/28/2020</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hopewell Essential Oils</td>\n",
       "      <td>Center for Drug Evaluation and Research | CDER</td>\n",
       "      <td>04/27/2020</td>\n",
       "      <td>04/28/2020</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        company_name  \\\n",
       "0  Health Solutions Pharmacy Center Inc dba Profe...   \n",
       "1                            Promex Distribution LLC   \n",
       "2                                 JusByJulie.Com LLC   \n",
       "3                                        T & S Dairy   \n",
       "4                            Hopewell Essential Oils   \n",
       "\n",
       "                                       issuing ofice letter_issue posted_date  \\\n",
       "0   Division of Pharmaceutical Quality Operations IV   03/31/2020  04/28/2020   \n",
       "1                      Division of Southeast Imports   04/13/2020  04/28/2020   \n",
       "2  Office of Human and Animal Food Operations Eas...   04/13/2020  04/28/2020   \n",
       "3  Division of Human and Animal Food Operations W...   04/14/2020  04/28/2020   \n",
       "4     Center for Drug Evaluation and Research | CDER   04/27/2020  04/28/2020   \n",
       "\n",
       "                                  warning_letter_url  \n",
       "0  /inspections-compliance-enforcement-and-crimin...  \n",
       "1  /inspections-compliance-enforcement-and-crimin...  \n",
       "2  /inspections-compliance-enforcement-and-crimin...  \n",
       "3  /inspections-compliance-enforcement-and-crimin...  \n",
       "4  /inspections-compliance-enforcement-and-crimin...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.21\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test_url = 'https://web.archive.org/web/20200406193325/https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/compliance-actions-and-activities/warning-letters'\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\"--incognito\")\n",
    "\n",
    "chromedriver = r'C:\\Users\\Jay M. Patel\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(chromedriver, options=option)\n",
    "browser.get(test_url)\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "element = browser.find_element_by_xpath('//*[@id=\"DataTables_Table_0_length\"]/label/select')\n",
    "\n",
    "select = Select(element)\n",
    "\n",
    "# select by visible text\n",
    "select.select_by_visible_text('All')\n",
    "\n",
    "posted_date_list = []\n",
    "letter_issue_list = []\n",
    "warning_letter_url = []\n",
    "company_name_list = []\n",
    "issuing_office_list = []\n",
    "\n",
    "soup_level1=BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "for tr in soup_level1.find_all('tr')[1:]:\n",
    "    tds = tr.find_all('td')\n",
    "    posted_date_list.append(tds[0].text)\n",
    "    letter_issue_list.append(tds[1].text)\n",
    "    warning_letter_url.append(tds[2].find('a')['href'])\n",
    "    company_name_list.append(tds[2].text)\n",
    "    issuing_office_list.append(tds[3].text)\n",
    "    #print(tds[0].text, tds[1].text,tds[2].find('a')['href'], tds[2].text, tds[3].text)\n",
    "browser.close()\n",
    "df = pd.DataFrame({'posted_date': posted_date_list,\n",
    "    'letter_issue': letter_issue_list,\n",
    "    'warning_letter_url': warning_letter_url,\n",
    "    'company_name': company_name_list,\n",
    "    'issuing ofice': issuing_office_list})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request code:  200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkresponse</th>\n",
       "      <th>field_associated_for_closeout_le</th>\n",
       "      <th>field_associated_for_response_le</th>\n",
       "      <th>field_building</th>\n",
       "      <th>field_change_date_2</th>\n",
       "      <th>field_company_name_warning_lette</th>\n",
       "      <th>field_detailed_description_2</th>\n",
       "      <th>field_issuing_office</th>\n",
       "      <th>field_issuing_office_loc_name</th>\n",
       "      <th>field_letter_issue_datetime</th>\n",
       "      <th>...</th>\n",
       "      <th>field_recipient_address_administrative_area</th>\n",
       "      <th>field_recipient_office_name</th>\n",
       "      <th>field_recipient_state</th>\n",
       "      <th>field_topics</th>\n",
       "      <th>issuing-recipient-office</th>\n",
       "      <th>path</th>\n",
       "      <th>term_node_tid</th>\n",
       "      <th>topics-product</th>\n",
       "      <th>abs_url</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Philadelphia District Office</td>\n",
       "      <td>01/13/2015</td>\n",
       "      <td>&lt;a href=\"/inspections-compliance-enforcement-a...</td>\n",
       "      <td>Low Acid Dog and Cat Canned Food Regulation/Ad...</td>\n",
       "      <td></td>\n",
       "      <td>Philadelphia District Office</td>\n",
       "      <td>01/02/2015</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Philadelphia District Office</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "      <td>Animal &amp;amp; Veterinary</td>\n",
       "      <td>Animal &amp;amp; Veterinary</td>\n",
       "      <td>https://www.fda.gov/inspections-compliance-enf...</td>\n",
       "      <td>Nestle Purina PetCare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Center for Devices and Radiological Health</td>\n",
       "      <td>01/19/2015</td>\n",
       "      <td>&lt;a href=\"/inspections-compliance-enforcement-a...</td>\n",
       "      <td>Medical Device Reporting/Misbranded</td>\n",
       "      <td></td>\n",
       "      <td>Center for Devices and Radiological Health</td>\n",
       "      <td>11/24/2015</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Center for Devices and Radiological Health</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.fda.gov/inspections-compliance-enf...</td>\n",
       "      <td>Sagami Rubber Industries Co., Ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Center for Tobacco Products</td>\n",
       "      <td>01/19/2015</td>\n",
       "      <td>&lt;a href=\"/inspections-compliance-enforcement-a...</td>\n",
       "      <td>Family Smoking Prevention and Tobacco Control ...</td>\n",
       "      <td></td>\n",
       "      <td>Center for Tobacco Products</td>\n",
       "      <td>01/07/2015</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Center for Tobacco Products</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.fda.gov/inspections-compliance-enf...</td>\n",
       "      <td>The Pipe Shop Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>New York District Office</td>\n",
       "      <td>01/19/2015</td>\n",
       "      <td>&lt;a href=\"/inspections-compliance-enforcement-a...</td>\n",
       "      <td>CGMP/Dietary Supplement/Adulterated/Misbranded</td>\n",
       "      <td></td>\n",
       "      <td>New York District Office</td>\n",
       "      <td>01/07/2015</td>\n",
       "      <td>...</td>\n",
       "      <td>New York</td>\n",
       "      <td></td>\n",
       "      <td>New York</td>\n",
       "      <td></td>\n",
       "      <td>New York District Office</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.fda.gov/inspections-compliance-enf...</td>\n",
       "      <td>NYSW Beverage Brands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Dallas District Office</td>\n",
       "      <td>01/19/2015</td>\n",
       "      <td>&lt;a href=\"/inspections-compliance-enforcement-a...</td>\n",
       "      <td>Illegal Drug Residue</td>\n",
       "      <td></td>\n",
       "      <td>Dallas District Office</td>\n",
       "      <td>01/09/2015</td>\n",
       "      <td>...</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td></td>\n",
       "      <td>Alaska</td>\n",
       "      <td></td>\n",
       "      <td>Dallas District Office</td>\n",
       "      <td>/inspections-compliance-enforcement-and-crimin...</td>\n",
       "      <td>Animal &amp;amp; Veterinary</td>\n",
       "      <td>Animal &amp;amp; Veterinary</td>\n",
       "      <td>https://www.fda.gov/inspections-compliance-enf...</td>\n",
       "      <td>Spurlock Farms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checkresponse field_associated_for_closeout_le  \\\n",
       "0                                                  \n",
       "1                                                  \n",
       "2                                                  \n",
       "3                                                  \n",
       "4                                                  \n",
       "\n",
       "  field_associated_for_response_le  \\\n",
       "0                                    \n",
       "1                                    \n",
       "2                                    \n",
       "3                                    \n",
       "4                                    \n",
       "\n",
       "                               field_building field_change_date_2  \\\n",
       "0                Philadelphia District Office          01/13/2015   \n",
       "1  Center for Devices and Radiological Health          01/19/2015   \n",
       "2                 Center for Tobacco Products          01/19/2015   \n",
       "3                    New York District Office          01/19/2015   \n",
       "4                      Dallas District Office          01/19/2015   \n",
       "\n",
       "                    field_company_name_warning_lette  \\\n",
       "0  <a href=\"/inspections-compliance-enforcement-a...   \n",
       "1  <a href=\"/inspections-compliance-enforcement-a...   \n",
       "2  <a href=\"/inspections-compliance-enforcement-a...   \n",
       "3  <a href=\"/inspections-compliance-enforcement-a...   \n",
       "4  <a href=\"/inspections-compliance-enforcement-a...   \n",
       "\n",
       "                        field_detailed_description_2 field_issuing_office  \\\n",
       "0  Low Acid Dog and Cat Canned Food Regulation/Ad...                        \n",
       "1                Medical Device Reporting/Misbranded                        \n",
       "2  Family Smoking Prevention and Tobacco Control ...                        \n",
       "3     CGMP/Dietary Supplement/Adulterated/Misbranded                        \n",
       "4                               Illegal Drug Residue                        \n",
       "\n",
       "                field_issuing_office_loc_name field_letter_issue_datetime  \\\n",
       "0                Philadelphia District Office                  01/02/2015   \n",
       "1  Center for Devices and Radiological Health                  11/24/2015   \n",
       "2                 Center for Tobacco Products                  01/07/2015   \n",
       "3                    New York District Office                  01/07/2015   \n",
       "4                      Dallas District Office                  01/09/2015   \n",
       "\n",
       "                  ...                  \\\n",
       "0                 ...                   \n",
       "1                 ...                   \n",
       "2                 ...                   \n",
       "3                 ...                   \n",
       "4                 ...                   \n",
       "\n",
       "   field_recipient_address_administrative_area field_recipient_office_name  \\\n",
       "0                                                                            \n",
       "1                                                                            \n",
       "2                                                                            \n",
       "3                                     New York                               \n",
       "4                                     Arkansas                               \n",
       "\n",
       "  field_recipient_state field_topics  \\\n",
       "0                                      \n",
       "1                                      \n",
       "2                                      \n",
       "3              New York                \n",
       "4                Alaska                \n",
       "\n",
       "                     issuing-recipient-office  \\\n",
       "0                Philadelphia District Office   \n",
       "1  Center for Devices and Radiological Health   \n",
       "2                 Center for Tobacco Products   \n",
       "3                    New York District Office   \n",
       "4                      Dallas District Office   \n",
       "\n",
       "                                                path            term_node_tid  \\\n",
       "0  /inspections-compliance-enforcement-and-crimin...  Animal &amp; Veterinary   \n",
       "1  /inspections-compliance-enforcement-and-crimin...                            \n",
       "2  /inspections-compliance-enforcement-and-crimin...                            \n",
       "3  /inspections-compliance-enforcement-and-crimin...                            \n",
       "4  /inspections-compliance-enforcement-and-crimin...  Animal &amp; Veterinary   \n",
       "\n",
       "            topics-product                                            abs_url  \\\n",
       "0  Animal &amp; Veterinary  https://www.fda.gov/inspections-compliance-enf...   \n",
       "1                           https://www.fda.gov/inspections-compliance-enf...   \n",
       "2                           https://www.fda.gov/inspections-compliance-enf...   \n",
       "3                           https://www.fda.gov/inspections-compliance-enf...   \n",
       "4  Animal &amp; Veterinary  https://www.fda.gov/inspections-compliance-enf...   \n",
       "\n",
       "                         company_name  \n",
       "0               Nestle Purina PetCare  \n",
       "1  Sagami Rubber Industries Co., Ltd.  \n",
       "2             The Pipe Shop Edinburgh  \n",
       "3                NYSW Beverage Brands  \n",
       "4                      Spurlock Farms  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code block 2.22\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "}\n",
    "\n",
    "test_url = 'https://web.archive.org/save/_embed/https://www.fda.gov/files/api/datatables/static/warning-letters.json?_=1586319220541'\n",
    "\n",
    "r = requests.get(url = test_url, headers = my_headers)\n",
    "print(\"request code: \", r.status_code)\n",
    "html_response = r.text\n",
    "string_json2 = io.StringIO(html_response)\n",
    "df = pd.read_json(string_json2)\n",
    "def get_abs_url(html_tag):\n",
    "    soup = BeautifulSoup(html_tag,'lxml')\n",
    "    abs_url = 'https://www.fda.gov' + soup.find('a')['href']\n",
    "    company_name = soup.find('a').get_text()\n",
    "    return abs_url, company_name\n",
    "df[\"abs_url\"], df[\"company_name\"] = zip(*df[\"field_company_name_warning_lette\"].apply(get_abs_url))\n",
    "df.to_csv(\"warning_letters_table.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
