{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: \n",
    "Listings 8-1 to 8-8 should be run in scrapy directory itself; they are provided here so that you better understand the code progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 8-1: default settings.py file contents\n",
    "BOT_NAME = 'chapter_8'\n",
    "SPIDER_MODULES = ['chapter_8.spiders']\n",
    "NEWSPIDER_MODULE = 'chapter_8.spiders'\n",
    "ROBOTSTXT_OBEY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-2: additional settings.py contents\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "\n",
    "CONCURRENT_REQUESTS = 1\n",
    "\n",
    "DOWNLOAD_DELAY = 0.05\n",
    "\n",
    "DOWNLOAD_TIMEOUT = 15\n",
    "\n",
    "REDIRECT_ENABLED = True\n",
    "\n",
    "DEPTH_LIMIT = 3\n",
    "DEPTH_PRIORITY = 1\n",
    "SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\n",
    "SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\n",
    "\n",
    "LOG_LEVEL = 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-3: items.py default contents\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class Chapter8Item(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-4: items.py fields\n",
    "import scrapy\n",
    "\n",
    "class Chapter8Item(scrapy.Item):\n",
    "    url = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    depth = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-5: linkscraper_basic.py default contents\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "class LinkscraperBasicSpider(scrapy.Spider):\n",
    "    name = 'linkscraper-basic'\n",
    "    allowed_domains = ['jaympatel.com']\n",
    "    start_urls = ['http://jaympatel.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-6: complete linkscraper_basic.py function.\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "from chapter_8.items import Chapter8Item\n",
    "\n",
    "class LinkscraperBasicSpider(scrapy.Spider):\n",
    "    name = 'linkscraper-basic'\n",
    "    allowed_domains = ['jaympatel.com']\n",
    "    start_urls = ['http://jaympatel.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    " \n",
    "            item = Chapter8Item()\n",
    "            if response.headers[\"Content-Type\"] == b'text/html; charset=utf-8' or response.headers[\"Content-Type\"] == b'text/html':\n",
    "                soup = BeautifulSoup(response.text,'html.parser')\n",
    "                urls = soup.find_all('a', href=True)\n",
    "                for val in soup.find_all('title'):\n",
    "                    try:\n",
    "                        item[\"url\"] = response.url\n",
    "                        item[\"title\"] = val.get_text()\n",
    "                        item[\"depth\"] = str(response.meta['depth'])\n",
    "                        yield item\n",
    "                    except Exception as E:\n",
    "                        print(str(E))\n",
    "                \n",
    "            else:\n",
    "                item[\"title\"] = 'title not extracted since content-type is ' + str(response.headers[\"Content-Type\"])\n",
    "                item[\"url\"] = response.url\n",
    "                item[\"depth\"] = str(response.meta['depth'])\n",
    "                urls = []\n",
    "                yield item\n",
    "            \n",
    "            \n",
    "            \n",
    "            for url in urls:\n",
    "                yield response.follow(url['href'], callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'http://jaympatel.com/', 'title': 'Jay M. Patel', 'depth': '0'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/tags/', 'title': 'Jay M. Patel', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/using-twitter-rest-apis-in-python-to-search-and-download-tweets-in-bulk/', 'title': '\\n    Using Twitter rest APIs in Python to search and download tweets in bulk – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/top-data-science-interview-questions-and-answers/', 'title': '\\n    Top data science interview questions and answers – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/', 'title': '\\n    Natural language processing (NLP): text vectorization and bag of words approach – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/categories/', 'title': 'Jay M. Patel', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python/', 'title': '\\n    Natural language processing (NLP): word embeddings, words2vec, GloVe based text vectorization in python – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python/', 'title': '\\n    Natural language processing (NLP): term frequency - inverse document frequency (Tf-idf) based vectorization in Python – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2018/11/get-started-with-git-and-github-in-under-10-minutes/', 'title': '\\n    Get started with Git and Github in under 10 Minutes – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification/', 'title': '\\n    Introduction to natural language processing: rule based methods, name entity recognition (NER), and text classification – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/08/how-to-create-pdf-documents-in-python-using-fpdf-library/', 'title': '\\n    How to create pdf documents in python using FPDF library – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup/', 'title': '\\n    Introduction to web scraping in python using Beautiful Soup – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/08/how-to-do-full-text-searching-in-python-using-whoosh-library/', 'title': '\\n    How to do full text searching in Python using Whoosh library – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping/', 'title': '\\n    Why is web scraping essential and who uses web scraping? – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/08/introduction-data-enrichment-based-on-email-addresses-and-domain-names/', 'title': '\\n    Introduction data enrichment based on email addresses and domain names – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/01/introduction-to-machine-learning-metrics/', 'title': '\\n    Introduction to machine learning metrics – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/books/', 'title': '\\n    Books – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/', 'title': 'Jay M. Patel', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/about/', 'title': '\\n    About – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/consulting-services/', 'title': '\\n    Consulting – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/cv/', 'title': '\\n    CV – Jay M. Patel\\n', 'depth': '2'}\n",
      "**********\n",
      "{'title': \"title not extracted since content-type is b'application/pdf'\", 'url': 'http://jaympatel.com/pages/CV.pdf', 'depth': '3'}\n",
      "**********\n",
      "{'title': \"title not extracted since content-type is b'application/pdf'\", 'url': 'http://jaympatel.com/assets/DoD_SERDP_case_study.pdf', 'depth': '3'}\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "#Listing 8-7: Exploring pages.jl file\n",
    "import json\n",
    "file_path = 'pages.jl'\n",
    "\n",
    "contents = open(file_path, \"r\").read()\n",
    "data = [json.loads(str(item)) for item in contents.strip().split('\\n')]\n",
    "for dd in data:\n",
    "    print(dd)\n",
    "    print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-8: additional settings.py parameters for using s3pipeline\n",
    "ITEM_PIPELINES = { 's3pipeline.S3Pipeline': 100}\n",
    "\n",
    "S3PIPELINE_URL = 's3://athena-us-east-1-testing/chapter-8/{time}.{chunk:07d}.jl.gz'\n",
    "\n",
    "S3PIPELINE_MAX_CHUNK_SIZE = 10000\n",
    "\n",
    "S3PIPELINE_GZIP = True\n",
    "\n",
    "# If different than AWS CLI configure values\n",
    "\n",
    "AWS_REGION_NAME = 'us-east-1'\n",
    "\n",
    "AWS_ACCESS_KEY_ID = ‘YOUR_VALUE’\n",
    "AWS_SECRET_ACCESS_KEY = ‘YOUR_VALUE’\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'http://jaympatel.com/', 'title': 'Jay M. Patel', 'depth': '0'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/', 'title': 'Jay M. Patel', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/about/', 'title': '\\n    About – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/consulting-services/', 'title': '\\n    Consulting – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/books/', 'title': '\\n    Books – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/01/introduction-to-machine-learning-metrics/', 'title': '\\n    Introduction to machine learning metrics – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup/', 'title': '\\n    Introduction to web scraping in python using Beautiful Soup – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping/', 'title': '\\n    Why is web scraping essential and who uses web scraping? – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/08/how-to-create-pdf-documents-in-python-using-fpdf-library/', 'title': '\\n    How to create pdf documents in python using FPDF library – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/08/how-to-do-full-text-searching-in-python-using-whoosh-library/', 'title': '\\n    How to do full text searching in Python using Whoosh library – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2020/08/introduction-data-enrichment-based-on-email-addresses-and-domain-names/', 'title': '\\n    Introduction data enrichment based on email addresses and domain names – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification/', 'title': '\\n    Introduction to natural language processing: rule based methods, name entity recognition (NER), and text classification – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/using-twitter-rest-apis-in-python-to-search-and-download-tweets-in-bulk/', 'title': '\\n    Using Twitter rest APIs in Python to search and download tweets in bulk – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python/', 'title': '\\n    Natural language processing (NLP): word embeddings, words2vec, GloVe based text vectorization in python – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/', 'title': '\\n    Natural language processing (NLP): text vectorization and bag of words approach – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python/', 'title': '\\n    Natural language processing (NLP): term frequency - inverse document frequency (Tf-idf) based vectorization in Python – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2019/02/top-data-science-interview-questions-and-answers/', 'title': '\\n    Top data science interview questions and answers – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/2018/11/get-started-with-git-and-github-in-under-10-minutes/', 'title': '\\n    Get started with Git and Github in under 10 Minutes – Jay M. Patel\\n', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/categories/', 'title': 'Jay M. Patel', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/tags/', 'title': 'Jay M. Patel', 'depth': '1'}\n",
      "**********\n",
      "{'url': 'http://jaympatel.com/cv/', 'title': '\\n    CV – Jay M. Patel\\n', 'depth': '2'}\n",
      "**********\n",
      "{'title': \"title not extracted since content-type is b'application/pdf'\", 'url': 'http://jaympatel.com/pages/CV.pdf', 'depth': '3'}\n",
      "**********\n",
      "{'title': \"title not extracted since content-type is b'application/pdf'\", 'url': 'http://jaympatel.com/assets/DoD_SERDP_case_study.pdf', 'depth': '3'}\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "#Listing 8-9: jl.gz output from S3 folder\n",
    "import gzip\n",
    "import json\n",
    "file_path_gzip= 'FILENAME_ON_S3.jl.gz'\n",
    "data = []\n",
    "with gzip.open(file_path_gzip,'r') as fin:        \n",
    "\n",
    "    for item in fin:\n",
    "        #print('got line', data.append(json.loads(item)))\n",
    "        data.append(json.loads(item))\n",
    "for dd in data:\n",
    "    print(dd)\n",
    "    print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-10: Modifying items.py file to capture raw web crawl data\n",
    "class Chapter8ItemRaw(scrapy.Item)\n",
    "    headers = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    response = scrapy.Field()\n",
    "    crawl_date = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 8-11: second-scraper.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from datetime import datetime, timezone\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from chapter_8.items import Chapter8ItemRaw\n",
    "\n",
    "\n",
    "class SecondScraperSpider(scrapy.Spider):\n",
    "    name = 'second-scraper'\n",
    "    allowed_domains = ['jaympatel.com']\n",
    "    start_urls = ['http://jaympatel.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        \n",
    "            item = Chapter8ItemRaw()\n",
    "            item['headers'] = str(response.headers)\n",
    "            item['url'] = response.url\n",
    "            item['body'] = response.text\n",
    "            item['crawl_date'] = datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
    "            yield item\n",
    "            \n",
    "            for a in LinkExtractor().extract_links(response):\n",
    "                yield response.follow(a, callback=self.parse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://jaympatel.com/\n",
      "Jay M. Patel\n",
      "**********\n",
      "http://jaympatel.com/\n",
      "Jay M. Patel\n",
      "**********\n",
      "http://jaympatel.com/about/\n",
      "\n",
      "    About – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/consulting-services/\n",
      "\n",
      "    Consulting – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/books/\n",
      "\n",
      "    Books – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2020/01/introduction-to-machine-learning-metrics/\n",
      "\n",
      "    Introduction to machine learning metrics – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup/\n",
      "\n",
      "    Introduction to web scraping in python using Beautiful Soup – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping/\n",
      "\n",
      "    Why is web scraping essential and who uses web scraping? – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2020/08/how-to-create-pdf-documents-in-python-using-fpdf-library/\n",
      "\n",
      "    How to create pdf documents in python using FPDF library – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2020/08/how-to-do-full-text-searching-in-python-using-whoosh-library/\n",
      "\n",
      "    How to do full text searching in Python using Whoosh library – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2020/08/introduction-data-enrichment-based-on-email-addresses-and-domain-names/\n",
      "\n",
      "    Introduction data enrichment based on email addresses and domain names – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification/\n",
      "\n",
      "    Introduction to natural language processing: rule based methods, name entity recognition (NER), and text classification – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/using-twitter-rest-apis-in-python-to-search-and-download-tweets-in-bulk/\n",
      "\n",
      "    Using Twitter rest APIs in Python to search and download tweets in bulk – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python/\n",
      "\n",
      "    Natural language processing (NLP): word embeddings, words2vec, GloVe based text vectorization in python – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/\n",
      "\n",
      "    Natural language processing (NLP): text vectorization and bag of words approach – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python/\n",
      "\n",
      "    Natural language processing (NLP): term frequency - inverse document frequency (Tf-idf) based vectorization in Python – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2019/02/top-data-science-interview-questions-and-answers/\n",
      "\n",
      "    Top data science interview questions and answers – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/2018/11/get-started-with-git-and-github-in-under-10-minutes/\n",
      "\n",
      "    Get started with Git and Github in under 10 Minutes – Jay M. Patel\n",
      "\n",
      "**********\n",
      "http://jaympatel.com/categories/\n",
      "Jay M. Patel\n",
      "**********\n",
      "http://jaympatel.com/tags/\n",
      "Jay M. Patel\n",
      "**********\n",
      "http://jaympatel.com/cv/\n",
      "\n",
      "    CV – Jay M. Patel\n",
      "\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "#Listing 8-12: Parsing jl.gz containing raw web crawls\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path_gzip= 'FILENAME_ON_S3.jl.gz'\n",
    "\n",
    "data = []\n",
    "with gzip.open(file_path_gzip,'r') as fin:        \n",
    "        for item in fin:\n",
    "            data.append(json.loads(item))\n",
    "for dd in data:\n",
    "    print(dd[\"url\"])\n",
    "    #print(dd[\"headers\"])\n",
    "    soup = BeautifulSoup(dd[\"response\"],'html.parser')\n",
    "    print(soup.find('title').get_text())\n",
    "    print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13622\n"
     ]
    }
   ],
   "source": [
    "# Listing 8-13: fetching amazon.com captures through cc-index api\n",
    "import urllib\n",
    "\n",
    "def get_index_url(query_url):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query_url)\n",
    "    base_url = 'https://index.commoncrawl.org/CC-MAIN-2020-16-index?url='\n",
    "    index_url = base_url + query + '&output=json'\n",
    "    return index_url\n",
    "query_url = 'amazon.com/*'\n",
    "index_url = get_index_url(query_url)\n",
    "\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "try:\n",
    "    from io import BytesIO\n",
    "except:\n",
    "    from StringIO import StringIO\n",
    "def get_index_json(index_url):\n",
    "    pages_list = []\n",
    "    #payload_content = None\n",
    "    \n",
    "    for i in range(4):\n",
    "        resp = requests.get(index_url)\n",
    "        #print(resp.status_code)\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            for x in resp.content.strip().decode().split('\\n'):\n",
    "                page = json.loads(x)\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    pages_list.append(page)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            break\n",
    "    return pages_list\n",
    "\n",
    "\n",
    "index_json = get_index_json(index_url)\n",
    "print(len(index_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 8-14: exploring status codes for amazon.com page captures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(index_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charset</th>\n",
       "      <th>digest</th>\n",
       "      <th>filename</th>\n",
       "      <th>languages</th>\n",
       "      <th>length</th>\n",
       "      <th>mime</th>\n",
       "      <th>mime-detected</th>\n",
       "      <th>offset</th>\n",
       "      <th>redirect</th>\n",
       "      <th>status</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>truncated</th>\n",
       "      <th>url</th>\n",
       "      <th>urlkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UTF-8</td>\n",
       "      <td>LVIFLF2KAXZEAFMDVTYY776KYNJROEPG</td>\n",
       "      <td>crawl-data/CC-MAIN-2020-16/segments/1585370490...</td>\n",
       "      <td>eng</td>\n",
       "      <td>66670</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>828564605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>20200328075237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/</td>\n",
       "      <td>com,amazon)/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Z6IJ46JXZU7TCLCDINT3OMVFHV5GZPYU</td>\n",
       "      <td>crawl-data/CC-MAIN-2020-16/segments/1585370490...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>711</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>4866000</td>\n",
       "      <td>https://www.amazon.com/</td>\n",
       "      <td>301</td>\n",
       "      <td>20200328081133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.amazon.com/</td>\n",
       "      <td>com,amazon)/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Z6IJ46JXZU7TCLCDINT3OMVFHV5GZPYU</td>\n",
       "      <td>crawl-data/CC-MAIN-2020-16/segments/1585370491...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>708</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>1180759</td>\n",
       "      <td>https://www.amazon.com/</td>\n",
       "      <td>301</td>\n",
       "      <td>20200328105528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.amazon.com/</td>\n",
       "      <td>com,amazon)/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>QOQN4QA34672OVBRYEUSI3RWPYEEYZWD</td>\n",
       "      <td>crawl-data/CC-MAIN-2020-16/segments/1585370491...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2778</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>18933473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>503</td>\n",
       "      <td>20200328124702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/</td>\n",
       "      <td>com,amazon)/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ</td>\n",
       "      <td>crawl-data/CC-MAIN-2020-16/segments/1585370492...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>498</td>\n",
       "      <td>unk</td>\n",
       "      <td>application/octet-stream</td>\n",
       "      <td>1226739</td>\n",
       "      <td>https://www.amazon.com/</td>\n",
       "      <td>301</td>\n",
       "      <td>20200328170442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.amazon.com</td>\n",
       "      <td>com,amazon)/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  charset                            digest  \\\n",
       "0   UTF-8  LVIFLF2KAXZEAFMDVTYY776KYNJROEPG   \n",
       "1     NaN  Z6IJ46JXZU7TCLCDINT3OMVFHV5GZPYU   \n",
       "2     NaN  Z6IJ46JXZU7TCLCDINT3OMVFHV5GZPYU   \n",
       "3     NaN  QOQN4QA34672OVBRYEUSI3RWPYEEYZWD   \n",
       "4     NaN  3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ   \n",
       "\n",
       "                                            filename languages length  \\\n",
       "0  crawl-data/CC-MAIN-2020-16/segments/1585370490...       eng  66670   \n",
       "1  crawl-data/CC-MAIN-2020-16/segments/1585370490...       NaN    711   \n",
       "2  crawl-data/CC-MAIN-2020-16/segments/1585370491...       NaN    708   \n",
       "3  crawl-data/CC-MAIN-2020-16/segments/1585370491...       NaN   2778   \n",
       "4  crawl-data/CC-MAIN-2020-16/segments/1585370492...       NaN    498   \n",
       "\n",
       "        mime             mime-detected     offset                 redirect  \\\n",
       "0  text/html                 text/html  828564605                      NaN   \n",
       "1  text/html                 text/html    4866000  https://www.amazon.com/   \n",
       "2  text/html                 text/html    1180759  https://www.amazon.com/   \n",
       "3  text/html                 text/html   18933473                      NaN   \n",
       "4        unk  application/octet-stream    1226739  https://www.amazon.com/   \n",
       "\n",
       "  status       timestamp truncated                      url        urlkey  \n",
       "0    200  20200328075237       NaN  https://www.amazon.com/  com,amazon)/  \n",
       "1    301  20200328081133       NaN   http://www.amazon.com/  com,amazon)/  \n",
       "2    301  20200328105528       NaN   http://www.amazon.com/  com,amazon)/  \n",
       "3    503  20200328124702       NaN  https://www.amazon.com/  com,amazon)/  \n",
       "4    301  20200328170442       NaN    http://www.amazon.com  com,amazon)/  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503    6753\n",
       "301    5274\n",
       "200     897\n",
       "302     635\n",
       "404      58\n",
       "400       5\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 8-14: exploring status codes for amazon.com page captures (cont.)\n",
    "\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 8-15: page with 503 status code\n",
    "page = df[df.status == '503'].iloc[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'charset': nan, 'digest': 'YK3ZEOTAVRL7MSJFY3IQHPCJOLVI4Q6Y', 'filename': 'crawl-data/CC-MAIN-2020-16/segments/1585370492125.18/crawldiagnostics/CC-MAIN-20200328164156-20200328194156-00241.warc.gz', 'languages': nan, 'length': '2592', 'mime': 'text/html', 'mime-detected': 'text/html', 'offset': '17355310', 'redirect': nan, 'status': '503', 'timestamp': '20200328181710', 'truncated': nan, 'url': 'https://www.amazon.com/', 'urlkey': 'com,amazon)/'}\n"
     ]
    }
   ],
   "source": [
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 8-15: page with 503 status code\n",
    "\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "try:\n",
    "    from io import BytesIO\n",
    "except:\n",
    "    from StringIO import StringIO\n",
    "\n",
    "def get_from_index(page):\n",
    "    \n",
    "    offset, length = int(page['offset']), int(page['length'])\n",
    "    offset_end = offset + length - 1\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    \n",
    "    try:\n",
    "\n",
    "        r = requests.get(prefix + page['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "        raw_data = BytesIO(r.content)\n",
    "        f = gzip.GzipFile(fileobj=raw_data)\n",
    "        data = f.read()\n",
    "      \n",
    "    except:\n",
    "\n",
    "        print('some error in connection?')\n",
    "\n",
    "    try:\n",
    "        crawl_metadata, header, response = data.strip().decode('utf-8').split('\\r\\n\\r\\n', 2)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        print(e)\n",
    "    \n",
    "    return crawl_metadata, header, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robot Check\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter the characters you see below\n",
      "Sorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Type the characters you see in this image:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Try different image\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Continue shopping\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conditions of Use\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Privacy Policy\n",
      "\n",
      "\n",
      "          © 1996-2014, Amazon.com, Inc. or its affiliates\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listing 8-15: page with 503 status code (Cont.)\n",
    "\n",
    "crawl_metadata, header, response = get_from_index(page)\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "for script in soup([\"script\", \"style\"]): \n",
    "        script.extract()\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 8-16: using proxy IP addresses with requests\n",
    "##NOTE: these proxy_ip addresses are just placeholders for explaining how the code works\n",
    "import requests\n",
    "\n",
    "proxy_ip = {\n",
    " 'http': 'http://11.11.11.11:8010',\n",
    " 'https': 'http://11.11.11.11:8010',\n",
    "}\n",
    "\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get(url, proxies=proxy_ip, headers = my_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "# Listing 8-17: Randomly generated user agent\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "print(ua.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 8-18: pseudocode for capchta solving service\n",
    "from selenium import webdriver\n",
    "browser = webdriver.Chrome\n",
    "\n",
    "captcha_site_key = browser.find_element_by_class_name('g-recaptcha').get_attribute('data-sitekey')\n",
    "#...(call captcha solving service API with site key and url\n",
    "# It will return back  g_response_code\n",
    "js_code = 'document.getElementById(\"g-recaptcha-response\").innerHTML = \"{}\";'.format(g_response_code)\n",
    "\n",
    "browser.execute_script(js_code)\n",
    "# Now perform whatever action you need to do on the page like hitting a submit button\n",
    "browser.find_element_by_tag_name('form').submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
